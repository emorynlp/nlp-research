# NAACL 2021: Dialog and Interactive Systems

https://2021.naacl.org/conference-program/main/program.html


## [Human-like informative conversations: Better acknowledgements using conditional mutual information](https://aclanthology.org/2021.naacl-main.61.pdf)

### Task

Dialogue response generation.

### Data

Topical Chat.

### Approach

Improve acknowledgements by selecting candidate response from finetuned GPT3 using PCMI between (1) candidate response and context conditioned on knowledge source, and (2) candidate response and knowledge source conditioned on context.

### Evaluation

Human judgement of response quality and attribution of quality improvement to acknowledgement.


## [A Comparative Study on Schema-Guided Dialogue State Tracking](https://aclanthology.org/2021.naacl-main.62.pdf)

### Task

Dialogue state tracking.

### Data

SG-DST, MultiWoZ 2.2.

### Approach

Bert encoding (1) dialogue token sequence and (2) natural language schemas defining intents and slots.

### Evaluation

Intent accuracy, slot F1. Evaluated different flavors of encoding schema (name only, short description, long description, etc.).


## [Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks](https://aclanthology.org/2021.naacl-main.63.pdf)

### Task

Intent detection and slot filling.

### Data

ATIS and Snips.

### Approach

BiLSTM with self attention encoder, and Key Value Memory Network (KV-MN) to store slot-value information. KV-MN adds, removes, and retrieves information from a value matrix using attention, similar to how LSTM adds and removes information from cell state. 

### Evaluation

Intent accuracy, slot F1, full sentence accuracy.


## [How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds](https://aclanthology.org/2021.naacl-main.64.pdf)

### Task

Play the game LIGHT by taking actions given game states and reward signals from the game engine. During play, the agent can communicate via natual language dialogue with a partner.

### Data

LIGHT text-based fantasy game, ATOMIC, Reddit.

### Approach

Value-based deep reinforcement learning (specifically, A3C) training a transfomer encoder. Encoding of the history of actions, environment descriptions, and dialogue turns is encoded by the transformer as a single sequence, then GRU layers are used to decode responses and actions. Rewards are given by a poly-encoder model trained on human demonstrations of playing the game, as well as direct rewards from the game engine. The conversation partner is a poly-encoder model pre trained on Reddit and fine tuned on human demonstrations. The partner and reward models do not update parameters during training.

### Evaluation

Goal completion rate in-game. Additionally, rate of outputting speech in either of (1) the top-k candidates of the reward model or (2) a response contained within the demonstration data.


## [Fine-grained Post-training for Improving Retrieval-based Dialogue Systems]()

### Task

Dialogue response retrieval/selection.

### Data

Ubuntu IRC Corpus V1, E-commerce Corpus, Douban Corpus.

### Approach

BERT encodes context and response candidate, and one final hidden vector used to classify candidate. After fine-tuning BERT in this way, a second post-training session was used that trains model to discriminate between correct and random response candidates using a shorter context window.

### Evaluation

Retrieval @K, mean average precision (MAP), mean reciprocal rank (MRR).


## [Put Chatbot into Its Interlocutorâ€™s Shoes: New Framework to Learn Chatbot Responding with Intention](https://aclanthology.org/2021.naacl-main.123.pdf)

### Task

Chat-oriented dialogue.

### Data

Empathetic Dialogues. 

### Approach

Fine tune DialoGPT using policy gradient reinforcment learning, where reward determined by length, specificity, and emotion of both response (as estimated by emotion classifier trained on emotion labels of ED) and follow-up response, where the follow-up to the system response is generated by another chatbot model.

### Evaluation

Perplexity, Self-BLEU (diversity metric), human judgements of coherence, and reward.


## [Adding Chit-Chat to Enhance Task-Oriented Dialogues](https://aclanthology.org/2021.naacl-main.124.pdf)

### Task

Task-oriented dialogue.

### Data

Schema-Guided Dialogue and MultiWOZ 2.1. Augmented task-oriented dialogue datasets with chit-chat (response segments appended or prepended to orignal response of a task-oriented dialogue).

### Approach

Combine off-the-shelf task oriented model with off-the-shelf chat oriented model using RoBERTa classifier to combine outputs of the two, then a GPT2 rewriter model to regenerate the response given that combination.

### Evaluation

Goal accuracy, slot F1, BLEU-4, and human judgements of engagingness, interestingness, knowledge, and humanness.


## [Bot-Adversarial Dialogue for Safe Conversational Agents](https://aclanthology.org/2021.naacl-main.235.pdf)

### Task

Chat-oriented dialogue, toxic language classification.

### Data

Bot-Adversarial Dialogue (BAD) created by authors.

### Approach

Add a "safety layer" to existing chatbot models (e.g. Blender, DialoGPT) that uses a toxic language classifier to prevent toxic response generation. Additionally, a preprocessing strategy using toxicity classifier was tried to encourage safe response generation.

### Evaluation

ACUTE evlauation of bot (human-bot convesations judged in pairwise compairson for quality by human), and F1 for evaluating unsafe/toxic language classifiers.


## [Example-Driven Intent Prediction with Observers](https://aclanthology.org/2021.naacl-main.237.pdf)

### Task

Task-oriented dialogue intent classification.

### Data

BANKING 77, CLINIC 150, and HWU 64. 

### Approach

Extension of BERT including "observers", which is an extension to the attention mechanism in which, at each attention layer, a hidden state (observer) attends to the other hidden states but is not attended to itself. Classification is performed by computing similarity between encoded utterances and encoded exemplars of each intent, in order to generalize model to new intents without retraining.

### Evaluation

Accuracy.


## [Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management](https://aclanthology.org/2021.naacl-main.238.pdf)

### Task

Intent, slot, and value prediction for task-oriented dialogue.

### Data

MultiWoZ.

### Approach

Inverse reinforcement learning of multi-level reward function to reward partially correct responses.

### Evaluation

Success rate, average turns to complete.


## [Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems](https://aclanthology.org/2021.naacl-main.239.pdf)

### Task

Given conversation history with a customer and a document describing customer service policies, bot must choose correct response-action out of a set of predefined options.

### Data

Action-Based Conversations Dataset (ABCD).

### Approach

Gave MTURKer "customers" short natural language prompt describing objective, and MTURKer "agents" an action GUI. Customer turkers engaged in dialogue with agent turkers in real time paired conversations. Customers communicated in natual language whereas agents could only select actions out of a predefined space.

### Evaluation

BERT model variants with classification and slot-filling architectures benchmarked on test split after supervised training. Action and slot filling accuracies, as well as task success measured.


## [Controlling Dialogue Generation with Semantic Exemplars](https://aclanthology.org/2021.naacl-main.240.pdf)

### Task

Chat-oriented dialogue response generation.

### Data

DailyDialog, FrameNet.

### Approach

Extension of TransferTransfo transformer model trained by conditioning generation on semantic frame of target response (frames extracted automatically with open-sesame model). During inference, retrieval of response examplars is used to select frame semantics, which then conditions generation.

### Evaluation

Dist-2, dist-3, MaUdE, and human rated coherence, fluency, consistency, interestingness, and usage of conditioned semantic frame(s).


## [Ensemble of MRR and NDCG models for Visual Dialog](https://aclanthology.org/2021.naacl-main.262.pdf)

### Task

Visual dialog (retrieval).

### Data

VisDial v1.0.

### Approach

Ensemble two ranking models, one optimized for MRR and one optimized for NDCG.

### Evaluation

MRR, R@K, NDCG.



## [CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues](https://aclanthology.org/2021.naacl-main.265.pdf)

### Task

Joint prediction of coreference links and query rewrites (for ellipsis resolution) within question-answering dialogue.

### Data

MuDoCo.

### Approach

Joint prediction of coreference and ellipsis resolutions using GPT2.

### Evaluation

For ellipses resolution: BLEU-4, precision, recall, F1, and whether coreferences were generated correctly within rewrites (comparing machine-generated tokens in the resolved-ellipsis-segment with that in the annotated resolved-ellipsis-segment). For coreference resolution: MUC, B^3, and CEAF.


## [Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems](https://aclanthology.org/2021.naacl-main.266.pdf)

### Task

Constraint violation detection of slot-filling in task-orinted dialogue.

### Data

MultiDoGO, extended with slot constraints.

### Approach

Deterministic and probabilistic constraint-checking approaches to detect violations in the NLU output of a intent classifier + slot filler model. Also tried a BERT classifier to do neural violation classification. 

### Evaluation

Precision, recall, F1, turn intersection-over-union (IoU) of violations.


## [Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition](https://aclanthology.org/2021.naacl-main.446.pdf)

### Task

Knowledge grounded chat-oriented dialogue.

### Data

Wizards of Wikipedia, DuConv corpus.

### Approach

Transformer attention between context and knowledge source.

### Evaluation

BLEU-1&2, Dist-1&2, human judgements of relevance, informativeness, and naturalness, also accuracy and F1 of shared knowledge.


## [Adversarial Self-Supervised Learning for Out-of-Domain Detection](https://aclanthology.org/2021.naacl-main.447.pdf)

### Task

Out of domain turn classification for task-oriented dialogue.

### Data

CLINIC.

### Approach

Train BLSTM encoding classifier on in-domain data and unlabeld OOD data.

### Evaluation

Accuracy and F1.


## [Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State Tracking](https://aclanthology.org/2021.naacl-main.448.pdf)

### Task

Zero shot learning of slot filling for dialogue state tracking.

### Data

MultiWOZ.

### Approach

Uses self-attention based encoder of context and new slot description.

### Evaluation

Joint goal accuracy, slot filling accuracy.


## [Hierarchical Transformer for Task Oriented Dialog Systems](https://aclanthology.org/2021.naacl-main.449.pdf)

### Task

Task-oriented dialogue.

### Data

MultiWOZ.

### Approach

Hierarchical transformer.

### Evaluation

BLEU, Entity-F1, inform and success.





